{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reproducible steps to create a corpus from [EDGAR](https://www.sec.gov/edgar) 10-k filings.\n",
    "\n",
    "# Example\n",
    "\n",
    "Retrieve a single file by hand.\n",
    "\n",
    "1. Navigate to [here](https://www.sec.gov/edgar/searchedgar/companysearch)\n",
    "2. Enter \"ENV\" into the search box.\n",
    "3. Right hand side, expand \"10-K (annual reports) and 10-Q (quarterly reports)\"\n",
    "4. Get whatever is on top.\n",
    "\n",
    "As of 2024/01/03\n",
    "\n",
    "\"ENV\" is:\n",
    "\n",
    "* https://www.sec.gov/edgar/browse/?CIK=1337619\n",
    "* https://www.sec.gov/ix?doc=/Archives/edgar/data/1337619/000133761923000012/env-20221231.htm\n",
    "\n",
    "\"MSFT\" is:\n",
    "\n",
    "* https://www.sec.gov/edgar/browse/?CIK=789019\n",
    "* https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000095017023035122/msft-20230630.htm\n",
    "\n",
    "\n",
    "# Pseudocode\n",
    "\n",
    "Below is a list of the steps we take.\n",
    "Keep in mind that these steps are a 10 thousand foot view.\n",
    "The implementation will be commented to a more detailed level.\n",
    "\n",
    "1. Get the tickers from the [SEC](https://www.sec.gov/file/company-tickers)\n",
    "2. Using the retrieved data, get the accession documents for the 10-Ks (`form_type`) the past 20 (`limit`) years\n",
    "3. Using the retrieved data, get the XHTML files\n",
    "4. For each retrieved XHTML, extract the \"Item 7: Management's Discussion ...\" section the a TXT file\n",
    "\n",
    "You can force a full re-download by deleting everything in _~/data/10-k_.\n",
    "Otherwise the script will do a checkpoint evaluation of how far it has processed.\n",
    "It will skip steps it thinks are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "tickers_url = 'https://www.sec.gov/files/company_tickers.json'\n",
    "user_agent = 'TextCorpusLabs/EDGAR'\n",
    "limit = 20\n",
    "form_type = '10-K'\n",
    "\n",
    "data_folder = Path('./data/')\n",
    "tickers_file = data_folder.joinpath('./tickers.csv')\n",
    "form_type_folder = data_folder.joinpath(f'./{form_type}')\n",
    "accessions_file = form_type_folder.joinpath('./accession.metadata.csv')\n",
    "raw_folder = form_type_folder.joinpath('./raw')\n",
    "corpus_folder = form_type_folder.joinpath('./corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "\n",
    "1. Get the list of tickers from the SEC\n",
    "2. Convert the tickers into an array, then sort it.\n",
    "3. Save the tickers to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_tickers(tickers_file: Path, tickers_url: str, user_agent: str, ) -> pd.DataFrame:\n",
    "    if not tickers_file.exists():\n",
    "        tickers = None\n",
    "        with requests.Session() as session:\n",
    "            session.headers['User-Agent'] = user_agent\n",
    "            with session.get(tickers_url) as result:\n",
    "                if result.status_code == 200:\n",
    "                    t1 = json.loads(result.text)\n",
    "                    t2 = [x for x in t1.values()]\n",
    "                    t3 = sorted(t2, key = lambda tup: tup['ticker'])\n",
    "                    tickers = [(x['cik_str'], x['ticker'], x['title']) for x in t3]\n",
    "        if tickers is not None:\n",
    "            df = pd.DataFrame(tickers, columns = ['CIK', 'Ticker', 'Name'])\n",
    "            if not tickers_file.parent.exists():\n",
    "                tickers_file.parent.mkdir(parents = True)\n",
    "            df.to_csv(tickers_file, index = False)\n",
    "        else:\n",
    "            raise RuntimeError('Error retrieving tickers')          \n",
    "    return pd.read_csv(tickers_file) #type: ignore\n",
    "\n",
    "tickers = get_tickers(tickers_file, tickers_url, user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "\n",
    "For each CIK in _tickers.csv_ (Step 1)\n",
    "\n",
    "1. Get the accessions for the past 20 10-Ks\n",
    "\n",
    "Save all the accessions for all the CIKs to disk\n",
    "\n",
    "**Note 1**: Notice `tickers.CIK.unique()`.\n",
    "The data pull needs to be done on CIK, not ticker.\n",
    "A single company can have more than one ticker (AACI vs AACIU), byt only one CIK (1844817).\n",
    "\n",
    "**Note 2**: Notice `except ValueError: pass`.\n",
    "It is possible for a CIK (or ticker) to have no associated documents of a particular type(10-k).\n",
    "`get_filing_metadatas()` responds to this case by throwing an error.\n",
    "On our side, it just means skip the record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type: ignore\n",
    "#cSpell: ignore tqdm, metadatas, dtype\n",
    "from sec_downloader import Downloader\n",
    "from tqdm.notebook import tqdm\n",
    "import sec_downloader.types as sec_t\n",
    "import typing as t\n",
    "\n",
    "def get_accession_metadata(accessions_file: Path, tickers: pd.DataFrame, form_type: str, limit: int, user_agent: str) -> pd.DataFrame:\n",
    "    if not accessions_file.exists():\n",
    "        metadata: t.List[sec_t.FilingMetadata] = []\n",
    "        downloader = Downloader(user_agent, '')\n",
    "        for cik in tqdm(tickers.CIK.unique()):\n",
    "            try:\n",
    "                t1 = downloader.get_filing_metadatas(sec_t.RequestedFilings(ticker_or_cik  = cik, form_type = form_type, limit = limit))\n",
    "                metadata.extend(t1)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if len(metadata) > 0:\n",
    "            df = pd.DataFrame(metadata)\n",
    "            df = df[[\"cik\", \"accession_number\", \"report_date\", \"primary_doc_url\"]]\n",
    "            df = df.rename(columns={'cik': 'CIK', 'accession_number': 'Accession Number', 'report_date': 'Report Date', 'primary_doc_url': 'URL'})\n",
    "            if not accessions_file.parent.exists():\n",
    "                accessions_file.parent.mkdir(parents = True)\n",
    "            df.to_csv(accessions_file, index = False)\n",
    "        else:\n",
    "            raise RuntimeError('Error retrieving accessions')\n",
    "    accessions = pd.read_csv(accessions_file, dtype = {'CIK': int, 'Accession Number': str, 'Report Date': str, 'URL': str})\n",
    "    accessions['Report Date'] = pd.to_datetime(accessions['Report Date'])\n",
    "    return accessions\n",
    "\n",
    "accessions = get_accession_metadata(accessions_file, tickers, form_type, limit, user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3\n",
    "\n",
    "For each accession in _accessions.csv_ (Step 2)\n",
    "\n",
    "1. Get the XHTML document\n",
    "2. save it to disk as _~/data/10-k/raw/{year}/{cik}.{accession number}.xhtml_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cSpell: ignore tqdm\n",
    "#type: ignore\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def get_accession_metadata(raw_folder: Path, accessions: pd.DataFrame, user_agent: str) -> None:\n",
    "    @dataclass\n",
    "    class Accession:\n",
    "        CIK: int\n",
    "        AccessionNumber: str\n",
    "        ReportDate: datetime\n",
    "        URL: str\n",
    "        def __init__(self, record: t.Dict[str, t.Union[int, str, datetime]]):\n",
    "            self.CIK = record['CIK']\n",
    "            self.AccessionNumber = record['Accession Number']\n",
    "            self.ReportDate = record['Report Date']\n",
    "            self.URL = record['URL']\n",
    "    def get_filing_xhtml(session: requests.Session, accession: Accession) -> t.Union[None, str]:\n",
    "        with session.get(accession.URL) as response:\n",
    "            if response.status_code == 200:\n",
    "                return response.text\n",
    "        return None\n",
    "    def get_xhtml_file_path(raw_folder: Path, accession: Accession) -> Path:\n",
    "        year = str(accession.ReportDate.year)\n",
    "        year = year if year != 'nan' else '0000'\n",
    "        return raw_folder.joinpath(f'{year}/{accession.CIK}.{accession.AccessionNumber}.xhtml')\n",
    "    with requests.Session() as session:\n",
    "        session.headers['User-Agent'] = user_agent\n",
    "        for accession in tqdm([Accession(x) for x in accessions.to_dict('records')]): \n",
    "            xhtml_file = get_xhtml_file_path(raw_folder, accession)\n",
    "            if not xhtml_file.parent.exists():\n",
    "                xhtml_file.parent.mkdir(parents = True)\n",
    "            if xhtml_file.exists():\n",
    "                continue\n",
    "            xhtml = get_filing_xhtml(session, accession)\n",
    "            if xhtml is None:\n",
    "                print(f'{accession.CIK}.{accession.AccessionNumber} failed')\n",
    "            else:\n",
    "                with open(xhtml_file, mode = 'w') as fp:\n",
    "                    fp.write(xhtml)\n",
    "\n",
    "get_accession_metadata(raw_folder, accessions, user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4\n",
    "\n",
    "For each XHTML document:\n",
    "\n",
    "1. Find \"Item 7: Management's Discussion ...\"\n",
    "2. Find the next section.\n",
    "3. Extract the IDs for both.\n",
    "4. Extract the HTML between the IDs\n",
    "5. Convert to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cSpell: ignore lxml, tqdm, xpaths\n",
    "#type: ignore\n",
    "from lxml import etree\n",
    "\n",
    "def extract_text(raw_folder: Path, corpus_folder: Path) -> None:\n",
    "    def get_txt_file_path(corpus_folder: Path, xhtml_file: Path) -> Path:\n",
    "        return corpus_folder.joinpath(xhtml_file.parent.name, xhtml_file.name.replace('.xhtml', '.txt'))\n",
    "    def get_ref_ids(node: etree.Element) -> t.Union[None, t.Tuple[str, str]]:\n",
    "        # you need to include the NS in every part of the `xpath`\n",
    "        # https://stackoverflow.com/questions/38936185/etree-xpath-return-entire-html-instead-of-text\n",
    "        ns_map = {'x':'http://www.w3.org/1999/xhtml'}\n",
    "        # older 10-ks have a different structure.\n",
    "        # they are missing the modern cross linking that we need\n",
    "        # around 2015, we start to see cross linking, but it is all over the place\n",
    "        # we find \"Financial Condition...\", go to its containing tr\n",
    "        # id1 is the the first href\n",
    "        # looking at all the following sibling trs, looking at the non-empty first tds, id2 is the first href\n",
    "        xpath_0 = \".//x:table/x:tr/x:td//*[contains(text(),'Financial Condition and Results of Operations')]/ancestor::x:tr\"\n",
    "        xpath_1 = \".//x:a[contains(@href, '#')]\"\n",
    "        xpath_2 = \"following-sibling::x:tr/x:td[1]//*[normalize-space(text())]/ancestor-or-self::x:a[contains(@href, '#')]\"\n",
    "        t0: list[etree.Element] = node.xpath(xpath_0, namespaces = ns_map)\n",
    "        if t0 is None or len(t0) == 0:\n",
    "            return None\n",
    "        t1: list[etree.Element] = t0[0].xpath(xpath_1, namespaces = ns_map)\n",
    "        t2: list[etree.Element] = t0[0].xpath(xpath_2, namespaces = ns_map)\n",
    "        if t1 is None or len(t1) == 0 or t2 is None or len(t2) == 0:\n",
    "            return None\n",
    "        ids = (t1[0].attrib['href'], t2[0].attrib['href'])\n",
    "        return [id[1:] for id in ids]\n",
    "    def get_text(node: etree.Element, ids: t.Tuple[str, str]) -> t.Union[None, str]:\n",
    "        ns_map = {'x':'http://www.w3.org/1999/xhtml'}\n",
    "        t1: list[etree.Element] = node.xpath(f\".//*\", namespaces = ns_map)\n",
    "        start: int = -1\n",
    "        end: int = -1\n",
    "        for i in range(0, len(t1)):\n",
    "            if 'id' in t1[i].attrib and t1[i].attrib['id'] == ids[0]:\n",
    "                start = i + 1\n",
    "                for j in range(i+1, len(t1)):\n",
    "                    if 'id' in t1[j].attrib and t1[j].attrib['id'] == ids[1]:\n",
    "                        end = j\n",
    "                        break\n",
    "                break\n",
    "        if start == -1 or end == -1:\n",
    "            return None\n",
    "        chunks = [elm.text for elm in t1[start:end] if elm.text is not None]\n",
    "        return '\\n'.join(chunks)\n",
    "    parser = etree.XMLParser(encoding = 'utf-8', recover = True, ns_clean = True)\n",
    "    for xhtml_file in tqdm([x for x in raw_folder.rglob('*.xhtml') if x.is_file()]):\n",
    "        txt_file = get_txt_file_path(corpus_folder, xhtml_file)\n",
    "        if txt_file.exists():\n",
    "            continue\n",
    "        with open(xhtml_file, mode = 'rb') as fp:\n",
    "            xhtml = fp.read()\n",
    "        root: etree.Element = etree.fromstring(xhtml, parser)\n",
    "        ids = get_ref_ids(root)\n",
    "        if ids is not None:\n",
    "            if not txt_file.parent.exists():\n",
    "                txt_file.parent.mkdir(parents = True)\n",
    "            text = get_text(root, ids)\n",
    "            if text is not None:\n",
    "                with open(txt_file, mode = 'w', encoding = 'utf-8') as fp:\n",
    "                    fp.write(text)\n",
    "extract_text(raw_folder, corpus_folder)"
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "minimal",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [],
   "report_row_ids": [],
   "version": 3
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
